{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See torchdyn optimal control tutorial and\n",
    "# [1] Optimal Energy Shaping via Neural Approximators: https://arxiv.org/abs/2101.05537\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad as grad\n",
    "\n",
    "TEST_RUN = True\n",
    "\n",
    "# physical parameters\n",
    "m, k, l, qr, b, g = 1., 0.5, 1, 0, 0.01, 9.81\n",
    "\n",
    "class ControlledSystem(nn.Module):\n",
    "    # Elastic Pendulum Model\n",
    "    def __init__(self, V, K):\n",
    "        super().__init__()\n",
    "        self.V, self.K, self.n = V, K, 1\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        # Evaluates the closed-loop vector field\n",
    "        with torch.set_grad_enabled(True):\n",
    "            q, p = x[..., :self.n], x[..., self.n:]\n",
    "            q = q.requires_grad_(True)\n",
    "            # compute control action\n",
    "            u = self._energy_shaping(q) + self._damping_injection(x)\n",
    "            # compute dynamics\n",
    "            dxdt = self._dynamics(q, p, u)\n",
    "        return dxdt\n",
    "\n",
    "    def _dynamics(self, q, p, u):\n",
    "        # controlled elastic pendulum dynamics\n",
    "        dqdt = p / m\n",
    "        dpdt = -k * (q - qr) - m * g * l * torch.sin(q) - b * p / m + u\n",
    "        return torch.cat([dqdt, dpdt], 1)\n",
    "\n",
    "    def _energy_shaping(self, q):\n",
    "        # energy shaping control action\n",
    "        dVdx = grad(self.V(q).sum(), q, create_graph=True)[0]\n",
    "        return -dVdx\n",
    "\n",
    "    def _damping_injection(self, x):\n",
    "        # damping injection control action\n",
    "        return -self.K(x) * x[:, self.n:] / m\n",
    "\n",
    "    def _autonomous_energy(self, x):\n",
    "        # Hamiltonian (total energy) of the UNCONTROLLED system\n",
    "        return (m * x[:, 1:] ** 2) / 2. + (k * (x[:, :1] - qr) ** 2) / 2 \\\n",
    "               + m * g * l * (1 - torch.cos(x[:, :1]))\n",
    "\n",
    "    def _energy(self, x):\n",
    "        # Hamiltonian (total energy) of the CONTROLLED system\n",
    "        return (m * x[:, 1:] ** 2) / 2. + (k * (x[:, :1] - qr) ** 2) / 2 \\\n",
    "               + m * g * l * (1 - torch.cos(x[:, :1])) + self.V(x[:, :1])\n",
    "\n",
    "\n",
    "class AugmentedDynamics(nn.Module):\n",
    "    # \"augmented\" vector field to take into account integral loss functions\n",
    "    def __init__(self, f, int_loss):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.int_loss = int_loss\n",
    "        self.nfe = 0.\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        self.nfe += 1\n",
    "        x = x[:,:2]\n",
    "        dxdt = self.f(t, x)\n",
    "        dldt = self.int_loss(t, x)\n",
    "        return torch.cat([dxdt, dldt], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.utils.data as data\n",
    "\n",
    "class EnergyShapingLearner(pl.LightningModule):\n",
    "    def __init__(self, model: nn.Module, prior_dist, target_dist, t_span, sensitivity='autograd'):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.prior, self.target = prior_dist, target_dist\n",
    "        self.t_span = t_span\n",
    "        self.batch_size = 2048\n",
    "        self.lr = 5e-3\n",
    "        self.weight = torch.Tensor([1., 1.]).reshape(1, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.odeint(x, self.t_span)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # sample a batch of initial conditions\n",
    "        x0 = self.prior.sample((self.batch_size,))\n",
    "\n",
    "        # Integrate the model\n",
    "        x0 = torch.cat([x0, torch.zeros(self.batch_size, 1).to(x0)], 1)\n",
    "        _, xTl = self(x0)\n",
    "        xT, l = xTl[-1, :, :2], xTl[-1, :, -1:]\n",
    "\n",
    "        # Compute loss\n",
    "        terminal_loss = weighted_log_likelihood_loss(xT, self.target, self.weight.to(xT))\n",
    "        integral_loss = torch.mean(l)\n",
    "        loss = terminal_loss + 0.01 * integral_loss\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=.999)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dummy_trainloader = data.DataLoader(\n",
    "            data.TensorDataset(torch.Tensor(1, 1), torch.Tensor(1, 1)),\n",
    "            batch_size=1)\n",
    "        return dummy_trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Uniform, Normal\n",
    "\n",
    "def prior_dist(q_min, q_max, p_min, p_max, device='cuda'):\n",
    "    # uniform \"prior\" distribution of initial conditions x(0)=[q(0),p(0)]\n",
    "    lb = torch.Tensor([q_min, p_min]).to(device)\n",
    "    ub = torch.Tensor([q_max, p_max]).to(device)\n",
    "    return Uniform(lb, ub)\n",
    "\n",
    "def target_dist(mu, sigma, device='cuda'):\n",
    "    # normal target distribution of terminal states x(T)\n",
    "    mu, sigma = torch.Tensor(mu).reshape(1, 2).to(device), torch.Tensor(sigma).reshape(1, 2).to(device)\n",
    "    return Normal(mu, torch.sqrt(sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_log_likelihood_loss(x, target, weight):\n",
    "    # weighted negative log likelihood loss\n",
    "    log_prob = target.log_prob(x)\n",
    "    weighted_log_p = weight * log_prob\n",
    "    return -torch.mean(weighted_log_p.sum(1))\n",
    "\n",
    "class ControlEffort(nn.Module):\n",
    "    # control effort integral cost\n",
    "    def __init__(self, f):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "    def forward(self, t, x):\n",
    "        with torch.set_grad_enabled(True):\n",
    "            q = x[:,:1].requires_grad_(True)\n",
    "            u = self.f._energy_shaping(q) + self.f._damping_injection(x)\n",
    "        return torch.abs(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import pi as pi\n",
    "\n",
    "prior = prior_dist(-2*pi, 2*pi, -2*pi, 2*pi) # Uniform \"prior\" distribution of initial conditions x(0) \n",
    "target = target_dist([0, 0], [.001, .001]) # Normal target distribution for x(T)\n",
    "\n",
    "# define optimal energy shaping policy networks as in [1]\n",
    "hdim = 64\n",
    "V = nn.Sequential(\n",
    "          nn.Linear(1, hdim),\n",
    "          nn.Softplus(), \n",
    "          nn.Linear(hdim, hdim),\n",
    "          nn.Tanh(), \n",
    "          nn.Linear(hdim, 1))\n",
    "K = nn.Sequential(\n",
    "          nn.Linear(2, hdim),\n",
    "          nn.Softplus(),\n",
    "          nn.Linear(hdim, 1),\n",
    "          nn.Softplus())\n",
    "\n",
    "for p in V[-1].parameters(): torch.nn.init.zeros_(p)\n",
    "for p in K[-2].parameters(): torch.nn.init.zeros_(p)\n",
    "\n",
    "# define controlled system dynamics\n",
    "f = ControlledSystem(V, K)\n",
    "aug_f = AugmentedDynamics(f, ControlEffort(f))\n",
    "\n",
    "# define time horizon\n",
    "# 3,30 default\n",
    "t_span = torch.linspace(0, 3, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdyn.models import ODEProblem\n",
    "prob = ODEProblem(aug_f, sensitivity='autograd', solver='rk4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | ODEProblem | 4.6 K \n",
      "-------------------------------------\n",
      "4.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 K     Total params\n",
      "0.018     Total estimated model params size (MB)\n",
      "c:\\Users\\kevin\\Desktop\\GitHub\\neural_dynamics_learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 24 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\kevin\\Desktop\\GitHub\\neural_dynamics_learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1555: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628de5f061bd4205afbb3ea8da519a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevin\\Desktop\\GitHub\\neural_dynamics_learning\\venv\\lib\\site-packages\\torchdyn\\numerics\\odeint.py:84: UserWarning: Setting tolerances has no effect on fixed-step methods\n",
      "  warn(\"Setting tolerances has no effect on fixed-step methods\")\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# train (it can be very slow on CPU) \n",
    "# (don't be scared if the loss starts very high)\n",
    "learn = EnergyShapingLearner(prob, prior, target, t_span)\n",
    "#trainer = pl.Trainer(max_epochs=650)\n",
    "if TEST_RUN:\n",
    "    trainer = pl.Trainer(max_epochs=1, accelerator=\"gpu\")\n",
    "else:\n",
    "    trainer = pl.Trainer(max_epochs=750, accelerator=\"gpu\")\n",
    "\n",
    "trainer.fit(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "n_ic = 256\n",
    "x0 = prior.sample(torch.Size([n_ic])).cpu()\n",
    "x0 = torch.cat([x0, torch.zeros(n_ic, 1)], 1)\n",
    "model = aug_f.cpu()\n",
    "#\n",
    "traj = odeint(model, x0, t_span, method='midpoint').detach()\n",
    "traj = traj[..., :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 256, 2])\n",
      "torch.Size([30, 256])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(traj.shape)\n",
    "traj_th = traj[:,:,0]\n",
    "print(traj_th.shape)\n",
    "np.savetxt(\"theta_trajectories.csv\", traj_th, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ec4f0d5b36b6683db12f8797e085cee6e12729370886e05b9ac2c1a9881565c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
